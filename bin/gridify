#!/usr/bin/env python -u

# gridify text, see --help

import sys
import argparse
from tabulate import tabulate
import codecs
import operator
import itertools
import csv

def parse_args():
    global args
    parser = argparse.ArgumentParser(description="""
Takes character-separated data in files or stdin and turns it into a grid,
just like mysql can do, using the tabulate[1] library.

Note: if --sepchar is a single space, multiple spaces will be collapsed into
a single space.

Examples:

    # chops all columns to 10 chars max, and only prints the 1st and 3rd column
    %(prog)s --truncate 10 --limit 1,3

    # examine input to determine useful options, this ignores all options
    # except for --format, --sepchar, and --truncate and replaces the normal
    # output with a few tables showing some stats about the input
    %(prog)s --examine

    # equivalent to `cut -f 1,3,7`, *except* that %(prog)s will output in the
    # exact order --limit says to, whereas `cut` sorts its -f argument
    %(prog)s --raw -l 7,1,3

[1] https://bitbucket.org/astanin/python-tabulate
""", formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('--limit', '-l',
        help='limit output to 1-based column indicies (default: none)')
    parser.add_argument('--format', '-f', default='psql',
        help='format to use for printing, see tabulate\'s options (default: psql)')
    parser.add_argument('--csv', action='store_true',
        help='parse input as csv format')
    parser.add_argument('--truncate', '-t', metavar='N', default=0, type=int,
        help='truncate column content to N letters')
    parser.add_argument('--sepchar', '-e', default='\t',
        help='separating character between fields (default: \\t)')
    parser.add_argument('--field-count', '-c', metavar='N', type=int,
        help='only include lines from input with N fields')
    parser.add_argument('--raw', '-r', action='store_true',
        help='raw output (do not use tabulate), ignores -f and -x')
    parser.add_argument('--examine', '-x', action='store_true',
        help='examine input for patterns (see description above)')

    args, remaining = parser.parse_known_args()
    args.files = remaining
    args.limit = [int(x) for x in args.limit.split(',')] if args.limit else []

def filter_fields(fields):
    if args.truncate > 0:
        fields = [v[:args.truncate] for v in fields]
    if not args.examine:
        if args.field_count > 0 and args.field_count != len(fields):
            return None
        if len(args.limit) > 0:
            indexes = filter(lambda i: i-1 < len(fields), args.limit)
            fields = [fields[i-1] for i in indexes]
    return fields

def get_lines(fh):
    if args.csv:
        reader = csv.reader(fh, dialect=args.csv)
        for row in reader:
            f = filter_fields(row)
            if f:
                yield f
    else:
        while True:
            line = fh.readline()
            if line == '':
                break
            if args.sepchar == ' ':
                fields = line.split()
            else:
                fields = [v.strip() for v in line.rstrip().split(args.sepchar)]
            f = filter_fields(fields)
            if f:
                yield f

def percentage(x, y):
    return 100 * float(x)/float(y)

def side_by_side(*args):
    paras = [x.splitlines() for x in args]
    widths = [len(max(p, key=len)) for p in paras]
    for x in list(itertools.izip_longest(*paras)):
        out = []
        for i,y in enumerate(x):
            if y is None:
                out.append(widths[i] * ' ')
            else:
                out.append(y)
        print(' '.join(out))

def pad_lines(lines, num_fields):
    for line in lines:
        yield line + [None] * (num_fields - len(line))

def read_file(fh):
    field_counts = {}
    total_lines = 0
    lines = []
    field_values = {}

    for fields in get_lines(fh):
        if args.raw:
            print(args.sepchar.join(fields))
            continue
        total_lines += 1
        num_fields = len(fields)
        lines.append(fields)

        if num_fields not in field_counts:
            field_counts[num_fields] = 0
        field_counts[num_fields] += 1

        for i,f in enumerate(fields):
            if i not in field_values:
                field_values[i] = set()
            field_values[i].add(f)

    if args.raw:
        return

    if not args.examine:
        print tabulate(pad_lines(lines, max(field_counts.keys(), key=int)), tablefmt=args.format)
        return

    field_counts = sorted([(
        k,
        v,
        percentage(v, total_lines)
    ) for k,v in field_counts.iteritems()], key=operator.itemgetter(2), reverse=True)[:20]

    field_values = sorted([(
        k+1,
        len(field_values[k]),
        percentage(len(field_values[k]), total_lines)
    ) for k in field_values.keys()], key=operator.itemgetter(0))[:20]

    table1 = tabulate([['Total lines', total_lines]], tablefmt=args.format)
    table2 = tabulate(field_counts, headers=['fields', 'freq', '%'],
        tablefmt=args.format, floatfmt='.1f')
    table3 = tabulate(field_values, headers=['index', 'uniqs', '%'],
        tablefmt=args.format, floatfmt='.1f')

    side_by_side(table3, table2, table1)

def read_files():
    if not args.files:
        read_file(sys.stdin)
    else:
        for f in args.files:
            if f == '-':
                read_file(sys.stdin)
            with codecs.open(f, 'r', 'utf-8') as fh:
                read_file(fh)

if __name__ == '__main__':
    parse_args()
    read_files()
